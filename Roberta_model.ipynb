{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be57c2cc-573c-4f56-b269-4ac43e9d8bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\kiril\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kiril\\anaconda3\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\kiril\\anaconda3\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\kiril\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.25.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.60.1)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\kiril\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn transformers tensorflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa07e1c9-852b-4910-842a-ad05b8843e03",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Phishing URL Detection with RoBERTa and TensorFlow\n",
    "\n",
    "This script demonstrates how to build a machine learning model to classify URLs as phishing or not phishing using the RoBERTa transformer model and TensorFlow. It includes steps from data loading to model training, saving, and evaluation.\n",
    "\n",
    "#### Data Preparation\n",
    "\n",
    "1. **Import Necessary Libraries**: We begin by importing required libraries including `pandas` for data manipulation, `sklearn.model_selection` for splitting the dataset, and components from `transformers` and `tensorflow` for model building.\n",
    "\n",
    "2. **Load Dataset**: \n",
    "    - `combined_df.csv` is loaded into a pandas DataFrame. This CSV file should contain at least two columns: one with URLs (`url`) and another with labels indicating whether each URL is phishing or not (`label`).\n",
    "\n",
    "3. **Split Dataset**:\n",
    "    - The dataset is split into training and testing sets using `train_test_split`, with 20% of the data reserved for testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a852ae8-9e98-4c4c-b564-2badf397b0f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kiril\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "                                                     url  label\n",
      "0                                            jandown.com      0\n",
      "1                             site9605278.92.webydo.com/      1\n",
      "2                                               popin.cc      0\n",
      "3                                            sporter.com      0\n",
      "4                        bisapersonalbo.webcindario.com/      1\n",
      "...                                                  ...    ...\n",
      "95563  docs.google.com/presentation/d/e/2PACX-1vSkb-F...      1\n",
      "95564                                        mobile9.com      0\n",
      "95565                               acheconcursos.com.br      0\n",
      "95566  shoutout.wix.com/so/8eOrt07Jg/c?w=W9FFCDF73qTE...      1\n",
      "95567                  galiciaseguridad.myportfolio.com/      1\n",
      "\n",
      "[95568 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kiril\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "csv_file_path = 'combined_df.csv'\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "combined_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "\n",
    "# Assuming combined_df is your DataFrame\n",
    "combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(combined_df)\n",
    "\n",
    "# Split dataset into training, validation, and testing sets\n",
    "train_val_df, test_df = train_test_split(combined_df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.25, random_state=42)  # Results in 60% train, 20% validation, 20% test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6271a71b-5196-44fc-a9b3-fe6a70194002",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "#### Model Preparation\n",
    "\n",
    "1. Tokenizer Initialization:\n",
    "    - A RoBERTa tokenizer is initialized to process the URLs, converting them into a format suitable for the model.\n",
    "\n",
    "2. Tokenization Function:\n",
    "    - Defines a function to tokenize the URLs. This function adjusts the padding and truncation to ensure consistent input size.\n",
    "\n",
    "3. Label Preparation:\n",
    "    - Converts the `label` column into a one-hot encoded format using `to_categorical`, facilitating binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1b0fa0e-76ac-44a1-b7db-17a976247636",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "250ed638edef45eb919e08492c844294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kiril\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kiril\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73351d0a777f42b6b32dd1ac68ed1f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbd8c66464748bdbd762ce37aa045b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# Tokenize URLs\n",
    "\n",
    "def tokenize_urls(texts, tokenizer, max_len=512):\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=max_len, return_tensors=\"tf\")\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize URLs for train, validation, and test sets\n",
    "train_encodings = tokenize_urls(train_df['url'].tolist(), tokenizer)\n",
    "val_encodings = tokenize_urls(val_df['url'].tolist(), tokenizer)\n",
    "test_encodings = tokenize_urls(test_df['url'].tolist(), tokenizer)\n",
    "\n",
    "# Prepare labels for train, validation, and test sets\n",
    "train_labels = to_categorical(train_df['label'])\n",
    "val_labels = to_categorical(val_df['label'])\n",
    "test_labels = to_categorical(test_df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd454e6e-c30f-490c-b795-6098c7096b81",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### RoBERTa Model Setup\n",
    "\n",
    "1. **Load Pre-Trained RoBERTa**:\n",
    "    - The pre-trained RoBERTa model is loaded with TensorFlow bindings. The model is set to non-trainable to utilize its pre-trained embeddings.\n",
    "\n",
    "2. **Model Architecture**:\n",
    "    - An input layer is defined for both input IDs and attention masks.\n",
    "    - RoBERTa's pooled output embeddings are extracted and passed through a dropout layer for regularization.\n",
    "    - A dense layer with softmax activation is used for binary classification.\n",
    "\n",
    "3. **Model Compilation**:\n",
    "    - The model is compiled with the Adam optimizer and categorical crossentropy loss function, suitable for binary classification tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9528c230-ec7d-45ab-920b-03b49ff37811",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ae425cc9854db2bab8672354fcf560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "Consistency check failed: file should be of size 498818054 but has size 154804332 (model.safetensors).\nWe are sorry for the inconvenience. Please retry download and pass `force_download=True, resume_download=False` as argument.\nIf the issue persists, please let us know by opening an issue on https://github.com/huggingface/huggingface_hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load pre-trained RoBERTa model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m roberta \u001b[38;5;241m=\u001b[39m TFRobertaModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta-base\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Freeze the RoBERTa model to reuse the pre-trained features without modifying them\u001b[39;00m\n\u001b[0;32m      5\u001b[0m roberta\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:2776\u001b[0m, in \u001b[0;36mTFPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2762\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m   2763\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   2764\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[0;32m   2765\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2774\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[0;32m   2775\u001b[0m     }\n\u001b[1;32m-> 2776\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs)\n\u001b[0;32m   2778\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[0;32m   2779\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[0;32m   2780\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m SAFE_WEIGHTS_NAME:\n\u001b[0;32m   2781\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py:428\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    427\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 428\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[0;32m    429\u001b[0m         path_or_repo_id,\n\u001b[0;32m    430\u001b[0m         filename,\n\u001b[0;32m    431\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[0;32m    432\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m    433\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    434\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    435\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m    436\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    437\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    438\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    439\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    440\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    447\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1364\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m temp_file_manager() \u001b[38;5;28;01mas\u001b[39;00m temp_file:\n\u001b[0;32m   1362\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownloading \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, temp_file\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m-> 1364\u001b[0m     http_get(\n\u001b[0;32m   1365\u001b[0m         url_to_download,\n\u001b[0;32m   1366\u001b[0m         temp_file,\n\u001b[0;32m   1367\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1368\u001b[0m         resume_size\u001b[38;5;241m=\u001b[39mresume_size,\n\u001b[0;32m   1369\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1370\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[0;32m   1371\u001b[0m     )\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblob_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:547\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[0;32m    544\u001b[0m         temp_file\u001b[38;5;241m.\u001b[39mwrite(chunk)\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expected_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m expected_size \u001b[38;5;241m!=\u001b[39m temp_file\u001b[38;5;241m.\u001b[39mtell():\n\u001b[1;32m--> 547\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    548\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsistency check failed: file should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but has size\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    549\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp_file\u001b[38;5;241m.\u001b[39mtell()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplayed_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWe are sorry for the inconvenience. Please retry download and\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    550\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m pass `force_download=True, resume_download=False` as argument.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf the issue persists, please let us\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    551\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m know by opening an issue on https://github.com/huggingface/huggingface_hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    552\u001b[0m     )\n\u001b[0;32m    554\u001b[0m progress\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mOSError\u001b[0m: Consistency check failed: file should be of size 498818054 but has size 154804332 (model.safetensors).\nWe are sorry for the inconvenience. Please retry download and pass `force_download=True, resume_download=False` as argument.\nIf the issue persists, please let us know by opening an issue on https://github.com/huggingface/huggingface_hub."
     ]
    }
   ],
   "source": [
    "\n",
    "# Load pre-trained RoBERTa model\n",
    "roberta = TFRobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "# Freeze the RoBERTa model to reuse the pre-trained features without modifying them\n",
    "roberta.trainable = False\n",
    "\n",
    "# Input layer\n",
    "input_ids = Input(shape=(None,), dtype='int32', name=\"input_ids\")\n",
    "attention_masks = Input(shape=(None,), dtype='int32', name=\"attention_mask\")\n",
    "\n",
    "# RoBERTa embeddings\n",
    "embeddings = roberta(input_ids, attention_mask=attention_masks)[1]  # We use the pooled output\n",
    "\n",
    "# Additional layers\n",
    "x = Dropout(0.1)(embeddings)\n",
    "output = Dense(2, activation='softmax')(x)  # Assuming binary classification (phishing or not)\n",
    "\n",
    "# Construct the model\n",
    "model = Model(inputs=[input_ids, attention_masks], outputs=output)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db995808-d264-4f04-8410-b2e456ac71d8",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "1. **Model Training**:\n",
    "    - The model is trained using the tokenized URL data and labels, with a validation split to monitor performance on unseen data.\n",
    "\n",
    "2. **Model Saving**:\n",
    "    - The trained model is saved to a directory for later use in prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9731455-70f3-48e6-a6b5-fdf029fce670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training to include validation data\n",
    "history = model.fit(\n",
    "    {'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']},\n",
    "    train_labels,\n",
    "    validation_data=(\n",
    "        {'input_ids': val_encodings['input_ids'], 'attention_mask': val_encodings['attention_mask']},\n",
    "        val_labels\n",
    "    ),\n",
    "    epochs=3  # Adjust epochs based on your dataset size and desired performance\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model_save_directory = 'my_roberta_model'\n",
    "model.save(model_save_directory, save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11063ce-a73d-4d2e-8dd9-dae0ee33d06b",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "1. **Model Loading**:\n",
    "    - Demonstrates how to load the saved model for further evaluation or prediction.\n",
    "\n",
    "2. **Prediction**:\n",
    "    - The script shows how to make predictions on new data, specifically on the test set URLs.\n",
    "\n",
    "3. **Evaluation Metrics**:\n",
    "    - Calculates and prints the accuracy and detailed classification report, providing insights into the model's performance on classifying URLs as phishing or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be856907-e533-4faa-a7bc-a208629c8760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model for prediction\n",
    "loaded_model = tf.keras.models.load_model(model_save_directory)\n",
    "\n",
    "# Make predictions\n",
    "predictions = loaded_model.predict(\n",
    "    {'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']}\n",
    ")\n",
    "\n",
    "# Convert predictions to label indices\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_df['label'].values, predicted_labels)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(classification_report(test_df['label'].values, predicted_labels, target_names=['Class 0', 'Class 1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6af0098-b0de-423c-92a0-dd11406c0839",
   "metadata": {},
   "source": [
    "## Model Training Visualization\n",
    "\n",
    "To understand how our model learns over time, we'll visualize its performance across epochs. We will plot both the accuracy and loss for the training and validation sets. This visualization helps in identifying key aspects of the training process, such as overfitting or underfitting, and whether the model is improving with each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c708c6-80cf-4189-9cb0-5bd4593a0535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['accuracy'], label='Train')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
